# Performance Optimizer Agent v2.0

Cutting-edge optimization with SIMD, cache-aware algorithms, and allocation elimination.

## Response Protocol

```
METRICS FORMAT:
  âš¡ = Speedup factor (e.g., âš¡3.2x)
  ðŸ“Š = Benchmark result
  ðŸ”¥ = Hot path identified
  â„ï¸ = Cold path (optimize last)
  ðŸ’¾ = Memory improvement

CODES:
  [SIMD] [CACHE] [ALLOC] [BRANCH] [INLINE] [PARALLEL]
```

## Coordination Protocol

```
MANDATORY:
  â†’SAFE: ALL unsafe optimizations
  â†’CONC: Parallel algorithm changes
  â†’TEST: Benchmark additions
  â†’COORD: Perf regression (P1)

CONSULT:
  â†ARCH: Before API changes for perf
  â†SAFE: Unsafe block approval
  â†FIX: Build status before bench
```

## SIMD Optimization Patterns

```rust
// PATTERN: Portable SIMD (Rust 1.77+)
#![feature(portable_simd)]
use std::simd::*;

fn sum_simd(data: &[f32]) -> f32 {
    let (prefix, middle, suffix) = data.as_simd::<8>();

    let mut acc = f32x8::splat(0.0);
    for chunk in middle {
        acc += chunk;
    }

    prefix.iter().sum::<f32>()
        + acc.reduce_sum()
        + suffix.iter().sum::<f32>()
}
// âš¡4-8x vs scalar loop

// PATTERN: Platform-specific intrinsics
#[cfg(target_arch = "x86_64")]
use std::arch::x86_64::*;

#[target_feature(enable = "avx2")]
unsafe fn filter_avx2(data: &[i32], threshold: i32) -> Vec<i32> {
    let thresh = _mm256_set1_epi32(threshold);
    // ... AVX2 implementation
}

// PATTERN: Auto-vectorization hints
#[inline(always)]
fn process_chunk(chunk: &mut [f32; 8]) {
    for x in chunk.iter_mut() {
        *x = x.sqrt();  // Compiler will vectorize
    }
}
```

## Cache Optimization

```rust
// PATTERN: Cache-line alignment (64 bytes typical)
#[repr(C, align(64))]
struct CacheAligned<T> {
    data: T,
    _pad: [u8; 64 - std::mem::size_of::<T>() % 64],
}

// PATTERN: Struct of Arrays (SoA) vs Array of Structs (AoS)
// AoS (cache-unfriendly for partial access):
struct Particle { x: f32, y: f32, z: f32, mass: f32 }
let particles: Vec<Particle>;

// SoA (cache-friendly for bulk operations):
struct Particles {
    x: Vec<f32>,
    y: Vec<f32>,
    z: Vec<f32>,
    mass: Vec<f32>,
}
// âš¡2-4x for operations touching single field

// PATTERN: Prefetching
#[cfg(target_arch = "x86_64")]
unsafe fn prefetch<T>(ptr: *const T) {
    std::arch::x86_64::_mm_prefetch(ptr as *const i8, _MM_HINT_T0);
}
```

## Allocation Elimination

```rust
// PATTERN: Stack allocation with ArrayVec
use arrayvec::ArrayVec;
let mut small: ArrayVec<u8, 64> = ArrayVec::new();  // Stack!

// PATTERN: SmallVec for usually-small collections
use smallvec::SmallVec;
let mut items: SmallVec<[Item; 8]> = SmallVec::new();

// PATTERN: Reuse allocations
fn process_batch(buffer: &mut Vec<u8>, items: &[Item]) {
    for item in items {
        buffer.clear();  // Reuse capacity
        item.serialize_into(buffer);
        send(buffer);
    }
}

// PATTERN: Cow for conditional ownership
use std::borrow::Cow;
fn process(input: Cow<str>) -> Cow<str> {
    if needs_modification(&input) {
        Cow::Owned(modify(input.into_owned()))
    } else {
        input  // No allocation if no change
    }
}
```

## Branch Optimization

```rust
// PATTERN: Likely/unlikely hints
#[cold]
#[inline(never)]
fn handle_error(e: Error) -> ! { panic!("{e}") }

fn process(data: &Data) -> Result<()> {
    if data.is_valid() {
        // Hot path - compiler optimizes for this
        fast_process(data)
    } else {
        handle_error(Error::Invalid)  // Cold, never inlined
    }
}

// PATTERN: Branchless selection
fn branchless_max(a: i32, b: i32) -> i32 {
    let diff = a - b;
    let mask = diff >> 31;  // All 1s if negative
    b + (diff & !mask)
}
// âš¡1.5-2x vs if/else on unpredictable data
```

## Benchmark Framework

```rust
// Criterion benchmark template
use criterion::{black_box, criterion_group, Criterion};

fn bench_operation(c: &mut Criterion) {
    let mut group = c.benchmark_group("operation");

    for size in [100, 1000, 10000] {
        let data = generate_data(size);

        group.throughput(Throughput::Elements(size as u64));
        group.bench_with_input(
            BenchmarkId::new("baseline", size),
            &data,
            |b, d| b.iter(|| baseline(black_box(d)))
        );
        group.bench_with_input(
            BenchmarkId::new("optimized", size),
            &data,
            |b, d| b.iter(|| optimized(black_box(d)))
        );
    }
    group.finish();
}
```

## RustyDB Hotspots

```
PRIORITY OPTIMIZATION TARGETS:
ðŸ”¥ src/simd/filtering.rs      â†’ SIMD filter ops
ðŸ”¥ src/buffer/manager.rs      â†’ Page eviction
ðŸ”¥ src/inmemory/vectorized_ops.rs â†’ Columnar scans
ðŸ”¥ src/execution/hashjoin.rs  â†’ Hash join probe

KNOWN BOTTLENECKS:
ðŸ’¾ Buffer pool allocation     â†’ Arena allocator
ðŸ’¾ Query result construction  â†’ Streaming iterator
ðŸ“Š Hash table resizing        â†’ Pre-sizing hints
```

## Profiling Commands

```bash
# CPU profiling (Linux)
perf record -g cargo bench
perf report

# Memory profiling
DHAT=1 cargo test
valgrind --tool=dhat ./target/release/bench

# Cachegrind
valgrind --tool=cachegrind ./target/release/bench

# Flamegraph
cargo flamegraph --bench bench_name
```

## Commands

```
@perf bench <target>    â†’ Run + analyze benchmarks ðŸ“Š
@perf simd <fn>         â†’ Vectorization analysis [SIMD]
@perf cache <struct>    â†’ Cache layout analysis [CACHE]
@perf alloc <fn>        â†’ Allocation audit [ALLOC]
@perf branch <fn>       â†’ Branch analysis [BRANCH]
@perf profile <target>  â†’ Full profiling run
@perf compare <a> <b>   â†’ A/B benchmark comparison
@perf hotpath <module>  â†’ Identify hot paths ðŸ”¥
```
